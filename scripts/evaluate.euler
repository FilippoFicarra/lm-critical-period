#!/bin/bash

#SBATCH -n 1
#SBATCH --cpus-per-task=8
#SBATCH --mem-per-cpu=8192
#SBATCH --gpus=4
#SBATCH --gres=gpumem:11264m
#SBATCH --time=23:00:00
#SBATCH --open-mode=truncate
#SBATCH --mail-type=END,FAIL


#! Model directory (i.e. where the model checkpoints are saved):
modeldir="/cluster/work/cotterell/iconstantine/lm-critical-period/checkpoints"

export WANDB_PROJECT="evaluation"
export WANDB__SERVICE_WAIT=300

#! Run options for the application:
case $TASK in

  blimp)
    workdir="/cluster/work/cotterell/iconstantine/evaluation-pipeline"
    prefix="python3"
    application="babylm_eval.py"
    options="$modeldir/${MODEL} ${MODEL_TYPE}"
    ;;

  glue)
    workdir="/cluster/work/cotterell/iconstantine/evaluation-pipeline"
    prefix=""
    application="finetune_all_tasks.sh"
    options="$modeldir/${MODEL} 5e-5 10 32"
    ;;

  fisher)
    workdir="/cluster/work/cotterell/iconstantine/lm-critical-period"
    prefix="MODEL_NAME=${MODEL} DATA_DIR=data/unified_clean/${LANG}  SEED=42"
    application="src/learn/eval_${MODEL_TYPE}.sh"
    options="--estimate_fisher_matrix"
    ;;

  l1)
    workdir="/cluster/work/cotterell/iconstantine/lm-critical-period"
    prefix="MODEL_NAME=${MODEL} DATA_DIR=data/unified_clean/${LANG}  SEED=42"
    application="src/learn/eval_${MODEL_TYPE}.sh"
    options=""
    ;;

  *)
    echo -n "unknown model name"
    ;;
esac


module purge
module load eth_proxy gcc/8.2.0 python_gpu/3.9.9
source $workdir/venv/bin/activate


cd $workdir
echo -e "Changed directory to `pwd`.\n"


echo -e "JobID: $SLURM_JOB_ID\n======"
echo "Time: `date`"
echo "Running on master node: `hostname`"
echo "Current directory: `pwd`"


CMD="$prefix $workdir/$application $options"
echo -e "\nExecuting command:\n==================\n$CMD\n"
eval $CMD
